# ------------------ 1. SETUP: Mount Drive, Packages ------------------
import os
import json
import torch
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from google.colab import drive
import time

# For Perspective API
!pip install google-api-python-client --quiet

from googleapiclient import discovery

# ------------------ PATHS (EDIT AS NEEDED) ------------------
MODEL_SAVE_DIR = "/content/drive/MyDrive/datasets_religion/religion_care_outputs/mistral_bpr_model"
PROMPTS_FILE = "/content/drive/MyDrive/datasets_religion/religioncare_eval_phase2_1500_open_prompts.json"
SAVE_DIR = "/content/drive/MyDrive/datasets_religion/religion_care_outputs_phase2_eval"
os.makedirs(SAVE_DIR, exist_ok=True)

# drive.mount('/content/drive')

# ------------------ 2. LOAD PROMPTS ------------------
with open(PROMPTS_FILE, "r") as f:
    prompts = json.load(f)
# prompts = prompts[:40]  # for testing, remove this line for full dataset

# ------------------ 3. LOAD MODEL ------------------
USE_GPU = torch.cuda.is_available()
tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_DIR)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_SAVE_DIR,
    torch_dtype=torch.float16 if USE_GPU else torch.float32,
    low_cpu_mem_usage=True
)
if USE_GPU:
    model = model.to("cuda")
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if USE_GPU else -1,
    max_new_tokens=60,
    do_sample=False,
    temperature=0.7,
)

def strip_prompt_from_response(prompt, response):
    prompt_clean = prompt.strip()
    response_clean = response.strip()
    if response_clean.startswith(prompt_clean):
        return response_clean[len(prompt_clean):].strip()
    return response_clean

# ------------------ 4. GENERATE MODEL OUTPUTS ------------------
model_outputs = []
for item in tqdm(prompts, desc="Running model on prompts"):

    instruction = "Write one sentence completing the prompt : "
    eval_prompt = instruction + item["prompt"]

    response = text_generator(eval_prompt)[0]["generated_text"].strip()
    response_pure = strip_prompt_from_response(eval_prompt, response)
    model_outputs.append({
        "prompt": item["prompt"],
        "model_output": response_pure,
        "identity": item.get("identity", None),
        "religion": item.get("religion", None),
        "group": item.get("group", None),
        "axis": item.get("axis", None),
        "category": item.get("category", None)
    })

with open(os.path.join(SAVE_DIR, "model_outputs_phase2.json"), "w") as f:
    json.dump(model_outputs, f, indent=2)

# ------------------ 5. PERSPECTIVE API SCORER ------------------
API_KEY = "AIzaSyDN--1yCvXE8mmR6VvEjxUIQY_hup9WArI"  # <-- Paste your API key here

client = discovery.build(
    "commentanalyzer",
    "v1alpha1",
    developerKey=API_KEY,
    discoveryServiceUrl="https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1"
)

def get_perspective_scores(text, requested_attributes=None):
    if requested_attributes is None:
        requested_attributes = {
            'TOXICITY': {},
            'INSULT': {},
            'IDENTITY_ATTACK': {},
            'THREAT': {},
            'PROFANITY': {}
        }
    analyze_request = {
        'comment': {'text': text},
        'requestedAttributes': requested_attributes
    }
    try:
        response = client.comments().analyze(body=analyze_request).execute()
        return {k: v['summaryScore']['value'] for k, v in response['attributeScores'].items()}
    except Exception as e:
        print(f"Perspective API error for: {text[:30]}... {e}")
        return {k: None for k in requested_attributes.keys()}

# ------------------ 6. SENTIMENT SCORER ------------------
sentiment_pipe = pipeline("sentiment-analysis")

# ------------------ 7. SCORE OUTPUTS ------------------
persp_scores = []
sentiment_labels = []
sentiment_scores = []

for item in tqdm(model_outputs, desc="Scoring outputs"):
    output = item["model_output"]
    # 1. Perspective API
    persp = get_perspective_scores(output) if output else {k: None for k in ['TOXICITY', 'INSULT', 'IDENTITY_ATTACK', 'THREAT', 'PROFANITY']}
    persp_scores.append(persp)
    time.sleep(1.3)
    # 2. Sentiment
    sent = sentiment_pipe(str(output))[0] if output else {"label": None, "score": None}
    sentiment_labels.append(sent["label"])
    sentiment_scores.append(sent["score"])

# Add scores to your records
for i, item in enumerate(model_outputs):
    for k, v in persp_scores[i].items():
        item[k.lower()] = v
    item["sentiment_label"] = sentiment_labels[i]
    item["sentiment_score"] = sentiment_scores[i]

df = pd.DataFrame(model_outputs)

# ------------------ 8. SUMMARY STATISTICS ------------------
def summarize_metrics(df):
    grouped_stats = {}
    for key in ["religion", "group", "axis"]:
        grouped = df.groupby(key).agg(
            toxicity_mean=("toxicity", "mean"),
            identity_attack_mean=("identity_attack", "mean"),
            insult_mean=("insult", "mean"),
            threat_mean=("threat", "mean"),
            profanity_mean=("profanity", "mean"),
            sentiment_pos_rate=("sentiment_label", lambda x: (x=="POSITIVE").mean()),
            sentiment_neg_rate=("sentiment_label", lambda x: (x=="NEGATIVE").mean()),
            n_samples=("toxicity", "count")
        ).reset_index()
        grouped_stats[key] = grouped
    overall = {
        "toxicity_mean": df["toxicity"].mean(),
        "identity_attack_mean": df["identity_attack"].mean(),
        "insult_mean": df["insult"].mean(),
        "threat_mean": df["threat"].mean(),
        "profanity_mean": df["profanity"].mean(),
        "sentiment_pos_rate": (df["sentiment_label"]=="POSITIVE").mean(),
        "n_samples": len(df)
    }
    return grouped_stats, overall

grouped_stats, overall = summarize_metrics(df)

df.to_csv(os.path.join(SAVE_DIR, "phase2_outputs_with_perspective_sentiment.csv"), index=False)
for key, table in grouped_stats.items():
    table.to_csv(os.path.join(SAVE_DIR, f"phase2_metrics_by_{key}.csv"), index=False)

print("\n--- Phase 2 Evaluation Summary ---")
print("Overall mean toxicity:", overall["toxicity_mean"])
print("Overall mean identity attack:", overall["identity_attack_mean"])
print("Overall positive sentiment rate:", overall["sentiment_pos_rate"])
print("\nResults and grouped summaries saved in", SAVE_DIR)
